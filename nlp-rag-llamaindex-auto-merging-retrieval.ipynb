{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lesson 4: Auto-merging Retrieval\n\n![Auto-merging retrieval.png](https://github.com/YanSte/NLP-RAG-LLamaIndex-Auto-Merging-Retrieval/blob/main/schema1.png?raw=true)\n![Auto-merging retrieval.png](https://github.com/YanSte/NLP-RAG-LLamaIndex-Auto-Merging-Retrieval/blob/main/schema2.png?raw=true)","metadata":{}},{"cell_type":"code","source":"import warnings\nfrom kaggle_secrets import UserSecretsClient\nwarnings.filterwarnings('ignore')\n\nuser_secrets = UserSecretsClient()\nOPENAI_API_KEY = user_secrets.get_secret(\"OPEN_AI\")","metadata":{"execution":{"iopub.status.busy":"2024-03-13T19:32:16.078112Z","iopub.execute_input":"2024-03-13T19:32:16.079457Z","iopub.status.idle":"2024-03-13T19:32:16.462487Z","shell.execute_reply.started":"2024-03-13T19:32:16.079411Z","shell.execute_reply":"2024-03-13T19:32:16.461297Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install trulens-eval\n!pip install llama-index\n!pip install llama-index-embeddings-huggingface\n!pip install torch sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-03-13T19:32:16.567178Z","iopub.execute_input":"2024-03-13T19:32:16.567580Z","iopub.status.idle":"2024-03-13T19:33:44.723824Z","shell.execute_reply.started":"2024-03-13T19:32:16.567550Z","shell.execute_reply":"2024-03-13T19:33:44.722251Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport openai\nopenai.api_key = OPENAI_API_KEY","metadata":{"execution":{"iopub.status.busy":"2024-03-13T19:37:12.420338Z","iopub.execute_input":"2024-03-13T19:37:12.420783Z","iopub.status.idle":"2024-03-13T19:37:19.812344Z","shell.execute_reply.started":"2024-03-13T19:37:12.420730Z","shell.execute_reply":"2024-03-13T19:37:19.811194Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to\n[nltk_data]     /opt/conda/lib/python3.10/site-\n[nltk_data]     packages/llama_index/legacy/_static/nltk_cache...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package punkt to\n[nltk_data]     /opt/conda/lib/python3.10/site-\n[nltk_data]     packages/llama_index/legacy/_static/nltk_cache...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n","output_type":"stream"}]},{"cell_type":"code","source":"from llama_index.core import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\n    input_files=[\"/kaggle/input/data-test/eBook-How-to-Build-a-Career-in-AI.pdf\"]\n).load_data()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:37:25.831554Z","iopub.execute_input":"2024-03-13T19:37:25.832863Z","iopub.status.idle":"2024-03-13T19:37:26.015470Z","shell.execute_reply.started":"2024-03-13T19:37:25.832803Z","shell.execute_reply":"2024-03-13T19:37:26.012283Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleDirectoryReader\n\u001b[0;32m----> 3\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleDirectoryReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/data-test/eBook-How-to-Build-a-Career-in-AI.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mload_data()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_index/core/readers/file/base.py:204\u001b[0m, in \u001b[0;36mSimpleDirectoryReader.__init__\u001b[0;34m(self, input_dir, input_files, exclude, exclude_hidden, errors, recursive, encoding, filename_as_id, required_exts, file_extractor, num_files_limit, file_metadata, fs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m input_files:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39misfile(path):\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    205\u001b[0m     input_file \u001b[38;5;241m=\u001b[39m Path(path)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_files\u001b[38;5;241m.\u001b[39mappend(input_file)\n","\u001b[0;31mValueError\u001b[0m: File /kaggle/input/data-test/eBook-How-to-Build-a-Career-in-AI.pdf does not exist."],"ename":"ValueError","evalue":"File /kaggle/input/data-test/eBook-How-to-Build-a-Career-in-AI.pdf does not exist.","output_type":"error"}]},{"cell_type":"code","source":"print(type(documents), \"\\n\")\nprint(len(documents), \"\\n\")\nprint(type(documents[0]))\nprint(documents[0])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:27:39.175282Z","iopub.status.idle":"2024-03-13T19:27:39.175795Z","shell.execute_reply.started":"2024-03-13T19:27:39.175614Z","shell.execute_reply":"2024-03-13T19:27:39.175632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Auto-merging retrieval setup","metadata":{}},{"cell_type":"code","source":"from llama_index import Document\n\ndocument = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:21:34.319390Z","iopub.status.idle":"2024-03-13T19:21:34.320184Z","shell.execute_reply.started":"2024-03-13T19:21:34.319925Z","shell.execute_reply":"2024-03-13T19:21:34.319949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hierarchical node parser.\n\nSplits a document into a recursive hierarchy Nodes using a NodeParser.\n\nNOTE: this will return a hierarchy of nodes in a flat list, where there will be overlap between parent nodes (e.g. with a bigger chunk size), and child nodes per parent (e.g. with a smaller chunk size).\n\nFor instance, this may return a list of nodes like: - list of top-level nodes with chunk size 2048 - list of second-level nodes, where each node is a child of a top-level node,\n\nchunk size 512\n\nlist of third-level nodes, where each node is a child of a second-level node, chunk size 128","metadata":{}},{"cell_type":"code","source":"from llama_index.node_parser import HierarchicalNodeParser\n\n# create the hierarchical node parser w/ default settings\nnode_parser = HierarchicalNodeParser.from_defaults(\n    chunk_sizes=[2048, 512, 128]\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:21:34.321545Z","iopub.status.idle":"2024-03-13T19:21:34.322605Z","shell.execute_reply.started":"2024-03-13T19:21:34.322361Z","shell.execute_reply":"2024-03-13T19:21:34.322380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nodes = node_parser.get_nodes_from_documents([document])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:21:34.323797Z","iopub.status.idle":"2024-03-13T19:21:34.324187Z","shell.execute_reply.started":"2024-03-13T19:21:34.323982Z","shell.execute_reply":"2024-03-13T19:21:34.323998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.node_parser import get_leaf_nodes\n\nleaf_nodes = get_leaf_nodes(nodes)\nprint(leaf_nodes[30].text)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:21:34.326103Z","iopub.status.idle":"2024-03-13T19:21:34.326481Z","shell.execute_reply.started":"2024-03-13T19:21:34.326323Z","shell.execute_reply":"2024-03-13T19:21:34.326337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nodes_by_id = {node.node_id: node for node in nodes}\n\nparent_node = nodes_by_id[leaf_nodes[30].parent_node.node_id]\nprint(parent_node.text)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:21:34.327808Z","iopub.status.idle":"2024-03-13T19:21:34.328111Z","shell.execute_reply.started":"2024-03-13T19:21:34.327967Z","shell.execute_reply":"2024-03-13T19:21:34.327978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building the index","metadata":{}},{"cell_type":"code","source":"from llama_index.llms import OpenAI\n\nllm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index import ServiceContext\n\nauto_merging_context = ServiceContext.from_defaults(\n    llm=llm,\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    node_parser=node_parser,\n)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index import VectorStoreIndex, StorageContext\n\nstorage_context = StorageContext.from_defaults()\nstorage_context.docstore.add_documents(nodes)\n\nautomerging_index = VectorStoreIndex(\n    leaf_nodes, storage_context=storage_context, service_context=auto_merging_context\n)\n\nautomerging_index.storage_context.persist(persist_dir=\"./merging_index\")","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This block of code is optional to check\n# if an index file exist, then it will load it\n# if not, it will rebuild it\n\nimport os\nfrom llama_index import VectorStoreIndex, StorageContext, load_index_from_storage\nfrom llama_index import load_index_from_storage\n\nif not os.path.exists(\"./merging_index\"):\n    storage_context = StorageContext.from_defaults()\n    storage_context.docstore.add_documents(nodes)\n\n    automerging_index = VectorStoreIndex(\n            leaf_nodes,\n            storage_context=storage_context,\n            service_context=auto_merging_context\n        )\n\n    automerging_index.storage_context.persist(persist_dir=\"./merging_index\")\nelse:\n    automerging_index = load_index_from_storage(\n        StorageContext.from_defaults(persist_dir=\"./merging_index\"),\n        service_context=auto_merging_context\n    )\n","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining the retriever and running the query engine","metadata":{}},{"cell_type":"code","source":"from llama_index.indices.postprocessor import SentenceTransformerRerank\nfrom llama_index.retrievers import AutoMergingRetriever\nfrom llama_index.query_engine import RetrieverQueryEngine\n\nautomerging_retriever = automerging_index.as_retriever(\n    similarity_top_k=12\n)\n\nretriever = AutoMergingRetriever(\n    automerging_retriever, \n    automerging_index.storage_context, \n    verbose=True\n)\n\nrerank = SentenceTransformerRerank(top_n=6, model=\"BAAI/bge-reranker-base\")\n\nauto_merging_engine = RetrieverQueryEngine.from_args(\n    automerging_retriever, node_postprocessors=[rerank]\n)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auto_merging_response = auto_merging_engine.query(\n    \"What is the importance of networking in AI?\"\n)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.response.notebook_utils import display_response\n\ndisplay_response(auto_merging_response)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Putting it all Together","metadata":{}},{"cell_type":"code","source":"import os\n\nfrom llama_index import (\n    ServiceContext,\n    StorageContext,\n    VectorStoreIndex,\n    load_index_from_storage,\n)\nfrom llama_index.node_parser import HierarchicalNodeParser\nfrom llama_index.node_parser import get_leaf_nodes\nfrom llama_index import StorageContext, load_index_from_storage\nfrom llama_index.retrievers import AutoMergingRetriever\nfrom llama_index.indices.postprocessor import SentenceTransformerRerank\nfrom llama_index.query_engine import RetrieverQueryEngine\n\n\ndef build_automerging_index(\n    documents,\n    llm,\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    save_dir=\"merging_index\",\n    chunk_sizes=None,\n):\n    chunk_sizes = chunk_sizes or [2048, 512, 128]\n    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)\n    nodes = node_parser.get_nodes_from_documents(documents)\n    leaf_nodes = get_leaf_nodes(nodes)\n    merging_context = ServiceContext.from_defaults(\n        llm=llm,\n        embed_model=embed_model,\n    )\n    storage_context = StorageContext.from_defaults()\n    storage_context.docstore.add_documents(nodes)\n\n    if not os.path.exists(save_dir):\n        automerging_index = VectorStoreIndex(\n            leaf_nodes, storage_context=storage_context, service_context=merging_context\n        )\n        automerging_index.storage_context.persist(persist_dir=save_dir)\n    else:\n        automerging_index = load_index_from_storage(\n            StorageContext.from_defaults(persist_dir=save_dir),\n            service_context=merging_context,\n        )\n    return automerging_index\n\n\ndef get_automerging_query_engine(\n    automerging_index,\n    similarity_top_k=12,\n    rerank_top_n=6,\n):\n    base_retriever = automerging_index.as_retriever(similarity_top_k=similarity_top_k)\n    retriever = AutoMergingRetriever(\n        base_retriever, automerging_index.storage_context, verbose=True\n    )\n    rerank = SentenceTransformerRerank(\n        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n    )\n    auto_merging_engine = RetrieverQueryEngine.from_args(\n        retriever, node_postprocessors=[rerank]\n    )\n    return auto_merging_engine","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.llms import OpenAI\n\nindex = build_automerging_index(\n    [document],\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n    save_dir=\"./merging_index\",\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query_engine = get_automerging_query_engine(index, similarity_top_k=6)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TruLens Evaluation","metadata":{}},{"cell_type":"code","source":"from trulens_eval import Tru\n\nTru().reset_database()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Two layers","metadata":{}},{"cell_type":"code","source":"auto_merging_index_0 = build_automerging_index(\n    documents,\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    save_dir=\"merging_index_0\",\n    chunk_sizes=[2048,512],\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auto_merging_engine_0 = get_automerging_query_engine(\n    auto_merging_index_0,\n    similarity_top_k=12,\n    rerank_top_n=6,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from utils import get_prebuilt_trulens_recorder\n\ntru_recorder = get_prebuilt_trulens_recorder(\n    auto_merging_engine_0,\n    app_id ='app_0'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_questions = []\nwith open('generated_questions.text', 'r') as file:\n    for line in file:\n        # Remove newline character and convert to integer\n        item = line.strip()\n        eval_questions.append(item)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_evals(eval_questions, tru_recorder, query_engine):\n    for question in eval_questions:\n        with tru_recorder as recording:\n            response = query_engine.query(question)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_evals(eval_questions, tru_recorder, auto_merging_engine_0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trulens_eval import Tru\n\nTru().get_leaderboard(app_ids=[])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tru().run_dashboard()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Three layers","metadata":{}},{"cell_type":"code","source":"auto_merging_index_1 = build_automerging_index(\n    documents,\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    save_dir=\"merging_index_1\",\n    chunk_sizes=[2048,512,128],\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auto_merging_engine_1 = get_automerging_query_engine(\n    auto_merging_index_1,\n    similarity_top_k=12,\n    rerank_top_n=6,\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tru_recorder = get_prebuilt_trulens_recorder(\n    auto_merging_engine_1,\n    app_id ='app_1'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_evals(eval_questions, tru_recorder, auto_merging_engine_1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trulens_eval import Tru\n\nTru().get_leaderboard(app_ids=[])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tru().run_dashboard()","metadata":{},"execution_count":null,"outputs":[]}]}