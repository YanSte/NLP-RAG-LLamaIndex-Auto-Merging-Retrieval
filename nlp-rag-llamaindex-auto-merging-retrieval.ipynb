{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lesson 4: Auto-merging Retrieval","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import utils\n\nimport os\nimport openai\nopenai.api_key = utils.get_openai_api_key()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\n    input_files=[\"./eBook-How-to-Build-a-Career-in-AI.pdf\"]\n).load_data()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(documents), \"\\n\")\nprint(len(documents), \"\\n\")\nprint(type(documents[0]))\nprint(documents[0])","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Auto-merging retrieval setup","metadata":{}},{"cell_type":"code","source":"from llama_index import Document\n\ndocument = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.node_parser import HierarchicalNodeParser\n\n# create the hierarchical node parser w/ default settings\nnode_parser = HierarchicalNodeParser.from_defaults(\n    chunk_sizes=[2048, 512, 128]\n)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nodes = node_parser.get_nodes_from_documents([document])","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.node_parser import get_leaf_nodes\n\nleaf_nodes = get_leaf_nodes(nodes)\nprint(leaf_nodes[30].text)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nodes_by_id = {node.node_id: node for node in nodes}\n\nparent_node = nodes_by_id[leaf_nodes[30].parent_node.node_id]\nprint(parent_node.text)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building the index","metadata":{}},{"cell_type":"code","source":"from llama_index.llms import OpenAI\n\nllm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index import ServiceContext\n\nauto_merging_context = ServiceContext.from_defaults(\n    llm=llm,\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    node_parser=node_parser,\n)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index import VectorStoreIndex, StorageContext\n\nstorage_context = StorageContext.from_defaults()\nstorage_context.docstore.add_documents(nodes)\n\nautomerging_index = VectorStoreIndex(\n    leaf_nodes, storage_context=storage_context, service_context=auto_merging_context\n)\n\nautomerging_index.storage_context.persist(persist_dir=\"./merging_index\")","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This block of code is optional to check\n# if an index file exist, then it will load it\n# if not, it will rebuild it\n\nimport os\nfrom llama_index import VectorStoreIndex, StorageContext, load_index_from_storage\nfrom llama_index import load_index_from_storage\n\nif not os.path.exists(\"./merging_index\"):\n    storage_context = StorageContext.from_defaults()\n    storage_context.docstore.add_documents(nodes)\n\n    automerging_index = VectorStoreIndex(\n            leaf_nodes,\n            storage_context=storage_context,\n            service_context=auto_merging_context\n        )\n\n    automerging_index.storage_context.persist(persist_dir=\"./merging_index\")\nelse:\n    automerging_index = load_index_from_storage(\n        StorageContext.from_defaults(persist_dir=\"./merging_index\"),\n        service_context=auto_merging_context\n    )\n","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining the retriever and running the query engine","metadata":{}},{"cell_type":"code","source":"from llama_index.indices.postprocessor import SentenceTransformerRerank\nfrom llama_index.retrievers import AutoMergingRetriever\nfrom llama_index.query_engine import RetrieverQueryEngine\n\nautomerging_retriever = automerging_index.as_retriever(\n    similarity_top_k=12\n)\n\nretriever = AutoMergingRetriever(\n    automerging_retriever, \n    automerging_index.storage_context, \n    verbose=True\n)\n\nrerank = SentenceTransformerRerank(top_n=6, model=\"BAAI/bge-reranker-base\")\n\nauto_merging_engine = RetrieverQueryEngine.from_args(\n    automerging_retriever, node_postprocessors=[rerank]\n)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auto_merging_response = auto_merging_engine.query(\n    \"What is the importance of networking in AI?\"\n)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.response.notebook_utils import display_response\n\ndisplay_response(auto_merging_response)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Putting it all Together","metadata":{}},{"cell_type":"code","source":"import os\n\nfrom llama_index import (\n    ServiceContext,\n    StorageContext,\n    VectorStoreIndex,\n    load_index_from_storage,\n)\nfrom llama_index.node_parser import HierarchicalNodeParser\nfrom llama_index.node_parser import get_leaf_nodes\nfrom llama_index import StorageContext, load_index_from_storage\nfrom llama_index.retrievers import AutoMergingRetriever\nfrom llama_index.indices.postprocessor import SentenceTransformerRerank\nfrom llama_index.query_engine import RetrieverQueryEngine\n\n\ndef build_automerging_index(\n    documents,\n    llm,\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    save_dir=\"merging_index\",\n    chunk_sizes=None,\n):\n    chunk_sizes = chunk_sizes or [2048, 512, 128]\n    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)\n    nodes = node_parser.get_nodes_from_documents(documents)\n    leaf_nodes = get_leaf_nodes(nodes)\n    merging_context = ServiceContext.from_defaults(\n        llm=llm,\n        embed_model=embed_model,\n    )\n    storage_context = StorageContext.from_defaults()\n    storage_context.docstore.add_documents(nodes)\n\n    if not os.path.exists(save_dir):\n        automerging_index = VectorStoreIndex(\n            leaf_nodes, storage_context=storage_context, service_context=merging_context\n        )\n        automerging_index.storage_context.persist(persist_dir=save_dir)\n    else:\n        automerging_index = load_index_from_storage(\n            StorageContext.from_defaults(persist_dir=save_dir),\n            service_context=merging_context,\n        )\n    return automerging_index\n\n\ndef get_automerging_query_engine(\n    automerging_index,\n    similarity_top_k=12,\n    rerank_top_n=6,\n):\n    base_retriever = automerging_index.as_retriever(similarity_top_k=similarity_top_k)\n    retriever = AutoMergingRetriever(\n        base_retriever, automerging_index.storage_context, verbose=True\n    )\n    rerank = SentenceTransformerRerank(\n        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n    )\n    auto_merging_engine = RetrieverQueryEngine.from_args(\n        retriever, node_postprocessors=[rerank]\n    )\n    return auto_merging_engine","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.llms import OpenAI\n\nindex = build_automerging_index(\n    [document],\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n    save_dir=\"./merging_index\",\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query_engine = get_automerging_query_engine(index, similarity_top_k=6)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TruLens Evaluation","metadata":{}},{"cell_type":"code","source":"from trulens_eval import Tru\n\nTru().reset_database()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Two layers","metadata":{}},{"cell_type":"code","source":"auto_merging_index_0 = build_automerging_index(\n    documents,\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    save_dir=\"merging_index_0\",\n    chunk_sizes=[2048,512],\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auto_merging_engine_0 = get_automerging_query_engine(\n    auto_merging_index_0,\n    similarity_top_k=12,\n    rerank_top_n=6,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from utils import get_prebuilt_trulens_recorder\n\ntru_recorder = get_prebuilt_trulens_recorder(\n    auto_merging_engine_0,\n    app_id ='app_0'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_questions = []\nwith open('generated_questions.text', 'r') as file:\n    for line in file:\n        # Remove newline character and convert to integer\n        item = line.strip()\n        eval_questions.append(item)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_evals(eval_questions, tru_recorder, query_engine):\n    for question in eval_questions:\n        with tru_recorder as recording:\n            response = query_engine.query(question)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_evals(eval_questions, tru_recorder, auto_merging_engine_0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trulens_eval import Tru\n\nTru().get_leaderboard(app_ids=[])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tru().run_dashboard()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Three layers","metadata":{}},{"cell_type":"code","source":"auto_merging_index_1 = build_automerging_index(\n    documents,\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    save_dir=\"merging_index_1\",\n    chunk_sizes=[2048,512,128],\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auto_merging_engine_1 = get_automerging_query_engine(\n    auto_merging_index_1,\n    similarity_top_k=12,\n    rerank_top_n=6,\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tru_recorder = get_prebuilt_trulens_recorder(\n    auto_merging_engine_1,\n    app_id ='app_1'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_evals(eval_questions, tru_recorder, auto_merging_engine_1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trulens_eval import Tru\n\nTru().get_leaderboard(app_ids=[])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tru().run_dashboard()","metadata":{},"execution_count":null,"outputs":[]}]}